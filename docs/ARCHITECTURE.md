# SEO Audit Pipeline - Architecture

This document provides a detailed overview of the system architecture for the Automated SEO Audit Pipeline. The architecture is designed for automation, scalability, and maintainability.

## Core Components

The pipeline consists of four main components that work together to automate the process of crawling websites and analyzing the data.

![Architecture Diagram](architecture.png)

### 1. Batch Crawler

-   **Technology**: PowerShell
-   **Script**: `scripts/run_crawler.ps1`
-   **Function**: This component is responsible for initiating the crawls. It reads a list of active domains from `config/domains.csv` and runs the Screaming Frog SEO Spider for each one using the command-line interface.
-   **Key Features**:
    -   **Parallel Processing**: Runs multiple Screaming Frog instances concurrently to significantly reduce the total time required to crawl all sites.
    -   **Configuration Driven**: All paths and settings are managed in `config/config.json`.
    -   **Error Handling**: Logs errors for any failed crawls without interrupting the entire batch process.
    -   **Organized Output**: Saves all CSV exports into a structured folder system: `exports/YYYY_MM_DD/domain.com/`.

### 2. ETL (Extract, Transform, Load) Pipeline

-   **Technology**: Python 3
-   **Script**: `scripts/run_etl.py`
-   **Function**: This is the data processing engine of the pipeline. It scans the `exports` directory for new crawl data, parses the relevant CSV files (specifically `internal_all.csv`), and loads the data into the PostgreSQL database.
-   **Key Features**:
    -   **Idempotent Design**: The script can be run multiple times without creating duplicate entries in the database.
    -   **Secure Credential Management**: Retrieves database credentials securely from environment variables or Windows Credential Manager, avoiding hardcoded passwords.
    -   **Robust Error Logging**: Logs all successes, warnings, and failures to the `etl_logs` table in the database for easy monitoring.
    -   **File Archiving**: Moves processed CSV folders to an `exports_archive` directory to prevent reprocessing.

### 3. PostgreSQL Database

-   **Technology**: PostgreSQL
-   **Schema**: `sql/01_create_schema.sql`
-   **Function**: The central repository for all structured data. It stores information about the sites, historical crawl metadata, page-level details, and aggregated issue summaries.
-   **Key Features**:
    -   **Normalized Schema**: Data is organized into related tables to reduce redundancy and improve data integrity. See `DATABASE.md` for more details.
    -   **Indexed for Performance**: Indexes are placed on foreign keys and frequently queried columns to ensure that reporting dashboards are fast and responsive.
    -   **Scalability**: PostgreSQL can handle large volumes of data, making it suitable for tracking many sites over a long period.

### 4. Backup & Sync

-   **Technology**: PowerShell, AWS CLI
-   **Script**: `scripts/run_backup.ps1`
-   **Function**: This component ensures the durability and safety of your data.
-   **Key Features**:
    -   **Database Dumps**: Automatically performs a `pg_dump` of the PostgreSQL database daily.
    -   **S3 Offloading**: Syncs both the raw CSV exports and the database backups to a specified AWS S3 bucket.
    -   **Automated Cleanup**: Deletes local database backups older than 30 days to manage disk space.

## Orchestration

-   **Technology**: Windows Task Scheduler
-   **Script**: `scripts/setup_scheduled_tasks.ps1`
-   **Function**: The entire pipeline is orchestrated using a series of scheduled tasks that run daily.
-   **Workflow**:
    1.  **2:00 AM**: The `run_crawler.ps1` script is triggered to start the crawls.
    2.  **~2:05 AM**: The `run_etl.py` script is triggered to process the files generated by the crawler.
    3.  **~2:10 AM**: The `run_backup.ps1` script is triggered to back up the newly updated database and raw files.

This chained, time-delayed approach ensures that each step runs only after the previous one has had time to complete.

## Data Flow

The data flows through the system as follows:

1.  **Initiation**: The `run_crawler.ps1` script reads the list of domains.
2.  **Crawling**: Screaming Frog crawls each site and outputs raw data as CSV files to the `exports` directory.
3.  **ETL**: The `run_etl.py` script finds these new CSVs, processes them, and inserts the data into the PostgreSQL database.
4.  **Archiving**: Once processed, the CSVs are moved to the `exports_archive` folder.
5.  **Backup**: The `run_backup.ps1` script dumps the database to a `.sql` file and syncs both the dump file and the original CSVs to AWS S3.
6.  **Reporting**: A BI tool (like Power BI) connects to the PostgreSQL database to visualize the data.
